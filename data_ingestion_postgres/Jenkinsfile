pipeline {
    agent any
    triggers {
        cron('H/5 * * * *') // H/5 means every 5 minutes, spread across available minutes
    }
    environment {
        VENV_DIR = '/var/lib/jenkins/venvs/inc_load_venv'
    }
    stages {
        stage('Install') {
            steps { 
                script {
                    sh '''
                        if [ ! -d "${VENV_DIR}" ]; then
                            echo "Creating virtual environment..."
                            python3 -m venv ${VENV_DIR}
                        else
                            echo "Using existing virtual environment..."
                        fi

                        source ${VENV_DIR}/bin/activate
                        echo "Installing dependencies..."
                        pip install --upgrade pip
                        pip install -r data_ingestion_postgres/requirements.txt
                    '''
                }
            }
        }
        stage('Run Python File for Incremental Load') {
            steps {
                 withCredentials([file(credentialsId: 'my-env-file', variable: 'ENV_FILE')]) {
                    sh '''
                        set -a
                        tr -d '\\r' < "$ENV_FILE" > temp_env && mv temp_env "$ENV_FILE"
                        source "$ENV_FILE"
                        set +a
                        source ${VENV_DIR}/bin/activate
                        python3 data_ingestion_postgres/src/etl_to_postgres.py inc
                    '''
                }
            }
        }
        stage('Run Bash Script for getting data from Postgres to Hive') {
            steps {
                withCredentials([file(credentialsId: 'my-env-file', variable: 'ENV_FILE')]) {
                    sh '''
                        set -a
                        tr -d '\\r' < "$ENV_FILE" > temp_env && mv temp_env "$ENV_FILE"
                        source "$ENV_FILE"
                        set +a
                        chmod +x data_ingestion_postgres/src/incremental_load_script.sh
                        data_ingestion_postgres/src/incremental_load_script.sh
                    '''
                }
            }
        }
    }
    post {
        success {
            // Output the for the current build
            echo "Build succeeded. The Inc job is completed successfully. Data ingestion to Postgres is done also from Postgres to Hive table."
        }
        failure {
            echo "Build failed. Please check the logs."
        }
        always {
            echo "Cleaning up workspace..."
            deleteDir()
        }
    }
}
