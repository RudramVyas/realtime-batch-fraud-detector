pipeline {
    agent any
    triggers {
        cron('H/5 * * * *') // H/5 means every 5 minutes, spread across available minutes
    }
    environment {
        VENV_DIR = 'venv'
    }
    stages {
        // stage('Checkout') { 
        //     steps { 
        //         checkout scm 
        //     } 
        // }
        stage('Install') {
            steps { 
                script {
                    sh '''
                        python3 -m venv ${VENV_DIR}
                        source ${VENV_DIR}/bin/activate
                        pip install -r requirements.txt
                    ''' 
                }
            }
        }
        stage('Run Python File for Incremental Load') {
            steps {
                 withCredentials([file(credentialsId: 'my-env-file', variable: 'ENV_FILE')]) {
                    sh '''
                        set -a
                        export ENV_FILE=$ENV_FILE
                        source $ENV_FILE
                        set +a
                        source ${VENV_DIR}/bin/activate
                        python3 src/etl_to_postgres.py inc
                    '''
                }
            }
        }
        stage('Run Bash Script for getting data from Postgres to Hive') {
            steps {
                script {
                    sh 'src/incremental_load_script.sh'
                }
            }
        }
        // stage('Test') {
        //     steps { 
        //         sh '''
        //         source venv/bin/activate
        //         pytest --junitxml=pytest.xml
        //         ''' 
        //     }
        // }
        // stage('Results') {
        //     steps { 
        //         junit 'pytest.xml' 
        //     }
        // }
    }
    post {
        // always { deleteDir() }
        success {
            // Output the for the current build
            echo "Build succeeded. The Inc job is completed successfully. Data ingestion to Postgres is done also from Postgres to Hive table."
        }
    }
}
